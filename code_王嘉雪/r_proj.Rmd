---
title: "r proj"
author: "Sasa"
date: "2023-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd(normalizePath("."))
```



```{r}
bili = read.csv('./bilibli_data.csv',header=TRUE)
head(bili,20)
```



### data preparing


离散变量设为factor：

```{r}
bili$sex <- factor(bili$sex)
bili$video_tag_combine <- factor(bili$video_tag_combine)

```


self_tags 转变为0，1类别变量

```{r}
bili[bili$self_tags=="[]", 18] <- 0
bili[bili$self_tags!= "0", 18] <- 1
bili$self_tags <- factor(bili$self_tags)

```


综合与第三方平台关联信息

```{r}
bili$num_platforms <- rowSums(bili[,11:17])
```



```{r}
data1 <- bili[, c(1,2:10,18:21)]
head(data1,5)
```


#### 典型的长尾分布


```{r}
library(ggplot2)

ggplot(data1, aes(x=follower))+geom_density(fill="#268AFF")+ ylim(0,5e-07)

```




```{r}
library(moments)
skewness(data1$follower)
```



###1 多元线性回归


Count outcome variables are sometimes log-transformed and analyzed using OLS regression. 
Many issues arise with this approach, including loss of data due to undefined values generated by taking the log of zero (which is undefined), as well as the lack of capacity to model the dispersion.



### Log transfromation


```{r}
data1 <- data1[data1$album != 0, ]
data1[, c(2,4,6,9,10)] <- log(data1[, c(2,4,6,9,10)])

skewness(data1$follower)

```


```{r, warning=FALSE}
library(cowplot)

p1 <- ggplot(data1, aes(x=video, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 

p2 <- ggplot(data1, aes(x=master, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 

p3 <- ggplot(data1, aes(x=album, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 

p4 <- ggplot(data1, aes(x=article, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 

p5 <- ggplot(data1, aes(x=channel, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 

p6 <- ggplot(data1, aes(x=time_ave20, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 

p7 <- ggplot(data1, aes(x=play_ave20, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 

p8 <- ggplot(data1, aes(x=video_max_ratio, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 

p9 <- ggplot(data1, aes(x=num_platforms, y=follower)) +
  geom_point(color='#268AFF') +
  geom_smooth(method=lm , color="#FD9A28", se=FALSE) 


plot_grid(p1,p2,p3,p4,p5,p6,p7,p8,p9)

```

play_ave20, num_platforms, album, master, video



```{r}
fit.linear <- lm(follower~play_ave20+video+num_platforms+album+master 
                   +video_tag_combine, data1)

summary(fit.linear);AIC(fit.linear)
```


F statistic is significant, so the model is useful;



*** stepBIC

```{r}
fitall <- lm(follower~., data1)
step(fit.linear, scope = formula(fitall), direction = 'both', k = log(8360) )

```



### fit.linear

```{r}
fit.linear <- lm(follower ~ play_ave20 + video + num_platforms + 
    album + master + video_max_ratio + time_ave20 + video_tag_combine,
    data = data1)

summary(fit.linear);AIC(fit.linear)
```


```{r}
library(car)
vif(fit.linear)
```




### Validate the assumptions


Normality:残差正态性，Q-Q plot;
Independence: observations are independent;
Linearity: there exist a linear relationship between the response variable and the predictors;
Homoscedasticity: constant variance assumption;



```{r}
library(car)

qqPlot(fit.linear)

```



```{r}
durbinWatsonTest(fit.linear)
```

The nonsignificant p-value suggests a lack of autocorrelation.




```{r, warning=FALSE}
crPlots(model = fit.linear)
```



```{r}
plot(fit.linear)
```




Non constant variance, 方差不齐


```{r}
ncvTest(fit.linear)
```



###### 方差不齐


### fit.gls


*** 广义最小二乘法（Generalized Least Squares, GLS）


```{r}
set.seed(1225)

fit.gls <- gls(follower ~ play_ave20 + video + num_platforms + 
    album + master + video_max_ratio + time_ave20+video_tag_combine, 
    data1, weights = varPower(), method = 'ML')

summary(fit.gls)
```



(Intercept)           -1.6250395
video_tag_combine鬼畜	-0.3674167
video_tag_combine科技	0.3940288
video_tag_combine其他	0.0031036
video_tag_combine生活	0.3671998	
video_tag_combine时尚	0.7057466
video_tag_combine舞蹈	0.3243540
video_tag_combine音乐	0.1428791
video_tag_combine影视	0.0416201
video_tag_combine游戏	-0.2573269
video_tag_combine娱乐	-0.3279960





### 模型解释


$$log(follower) =intercept+0.77*log(playave20)+0.6*log(video)+ 0.17*numplatforms+\\
0.05*log(album)+0.05*master+0.43*videomaxratio+0.14*log(timeave20)$$




### fit.wls


*** 加权最小二乘法（Weighted Least Squares, WLS）


```{r}
wights <- 1/(fit.linear$residuals)^2
```


```{r}
library(nlme)
fit.wls <- lm(follower ~ play_ave20 + video + num_platforms + 
    album + master + video_max_ratio + time_ave20+video_tag_combine, 
    data1, wights)
summary(fit.wls);AIC(fit.wls)
```





### 模型解释


$$log(follower) =intercept+0.65*log(playave20)+0.39*log(video)+ 0.48*numplatforms+\\
0.27*log(album)+0.05*master-0.09*videomaxratio-0.04*log(timeave20)$$






###2 广义线性模型


```{r}
data2 <- data1
data2$follower <- exp(data1$follower)
```


*** count data, Poisson or negative binomial


```{r}
mean(data2$follower); var(data2$follower)
```

The mean of our outcome variable is much lower than its variance.

However, for poisson regression, the variance of the outcome is equal to the mean.


```{r}
library(qcc)
qcc.overdispersion.test(data2$follower,type = 'poisson')

```

Not surprisingly, the significance test has a p-value less than 0.05, strongly suggesting the
presence of overdispersion.



#### negative binomial regression


It can be considered as a generalization of Poisson regression since it has the same mean structure as Poisson regression and it has an extra parameter to model the over-dispersion.


```{r}
library(MASS)
fit.nb <- glm.nb(follower ~ play_ave20 + video + num_platforms + 
    album + master + video_max_ratio + time_ave20+video_tag_combine, data2)
summary(fit.nb)

```




### 模型解释


The form of the model equation for negative binomial regression is the same as that for Poisson regression. The log of the outcome is predicted with a linear combination of the predictors:


$$log(follower)= intercept+0.72*log(playave20)+0.6*log(video)+0.14*numplatforms+\\0.03*log(album)
+0.02*master+0.08*videomaxratio+0.1*log(timeave20) $$













